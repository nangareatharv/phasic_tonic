{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the datasets\n",
    "\n",
    "The path to dataset directory and patterns to search in those directories for the HPC, PFC recordings are in loaded from the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DatasetLoader.__init__() missing 1 required positional argument: 'CONFIG_DIR'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 29\u001b[0m\n\u001b[0;32m     25\u001b[0m logger \u001b[38;5;241m=\u001b[39m logger_setup()\n\u001b[0;32m     27\u001b[0m CONFIG_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/nero/phasic_tonic/data/dataset_loading.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 29\u001b[0m Datasets \u001b[38;5;241m=\u001b[39m \u001b[43mDatasetLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m mapped_datasets \u001b[38;5;241m=\u001b[39m Datasets\u001b[38;5;241m.\u001b[39mload_datasets()\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(signal: np\u001b[38;5;241m.\u001b[39mndarray, n_down: \u001b[38;5;28mint\u001b[39m, target_fs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n",
      "\u001b[1;31mTypeError\u001b[0m: DatasetLoader.__init__() missing 1 required positional argument: 'CONFIG_DIR'"
     ]
    }
   ],
   "source": [
    "from phasic_tonic.detect_phasic import detect_phasic\n",
    "from phasic_tonic.DatasetLoader import DatasetLoader\n",
    "from phasic_tonic.helper import get_metadata\n",
    "from phasic_tonic.runtime_logger import logger_setup\n",
    "from phasic_tonic.utils import get_sequences\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pynapple as nap\n",
    "import yasa\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.io import loadmat\n",
    "from mne.filter import resample\n",
    "\n",
    "fs_cbd = 2500\n",
    "fs_os = 2500\n",
    "fs_rgs = 1000\n",
    "\n",
    "targetFs = 500\n",
    "n_down_cbd = fs_cbd/targetFs\n",
    "n_down_rgs = fs_rgs/targetFs\n",
    "n_down_os = fs_os/targetFs\n",
    "\n",
    "logger = logger_setup()\n",
    "\n",
    "CONFIG_DIR = \"/home/nero/phasic_tonic/data/dataset_loading.yaml\"\n",
    "\n",
    "Datasets = DatasetLoader(CONFIG_DIR)\n",
    "mapped_datasets = Datasets.load_datasets()\n",
    "\n",
    "def preprocess(signal: np.ndarray, n_down: int, target_fs=500) -> np.ndarray:\n",
    "    \"\"\"Downsample and remove artifacts.\"\"\"\n",
    "    \n",
    "    logger.debug(\"STARTED: Resampling to 500 Hz.\")\n",
    "    # Downsample to 500 Hz\n",
    "    data = resample(signal, down=n_down, method='fft', npad='auto')\n",
    "    logger.debug(\"FINISHED: Resampling to 500 Hz.\")\n",
    "    logger.debug(\"Resampled: {0} -> {1}.\".format(str(signal.shape), str(data.shape)))\n",
    "    \n",
    "    logger.debug(\"STARTED: Remove artifacts.\")\n",
    "    # Remove artifacts\n",
    "    art_std, _ = yasa.art_detect(data, target_fs , window=1, method='std', threshold=4)\n",
    "    art_up = yasa.hypno_upsample_to_data(art_std, 1, data, target_fs)\n",
    "    data[art_up] = 0\n",
    "    logger.debug(\"FINISHED: Remove artifacts.\")\n",
    "        \n",
    "    data -= data.mean()\n",
    "    return data\n",
    "\n",
    "def get_start_end(hypno: np.ndarray, sleep_state_id: int):\n",
    "    \"\"\"Convert sleep states into lists of start and end time indices.\"\"\"\n",
    "    seq = get_sequences(np.where(hypno == sleep_state_id)[0])\n",
    "    start = []\n",
    "    end = []\n",
    "    for s, e in seq:\n",
    "        start.append(s)\n",
    "        end.append(e)\n",
    "    return (start, end)\n",
    "\n",
    "def _detect_troughs(signal, thr):\n",
    "    lidx  = np.where(signal[0:-2] > signal[1:-1])[0]\n",
    "    ridx  = np.where(signal[1:-1] <= signal[2:])[0]\n",
    "    thidx = np.where(signal[1:-1] < thr)[0]\n",
    "    sidx = np.intersect1d(lidx, np.intersect1d(ridx, thidx))+1\n",
    "    return sidx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of loaded recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbd_cnt = 0\n",
    "rgs_cnt = 0\n",
    "os_cnt = 0\n",
    "\n",
    "# Count recordings belonging to CBD dataset\n",
    "for name in mapped_datasets:\n",
    "    metadata = get_metadata(name)\n",
    "    if metadata['treatment'] == 0 or metadata['treatment'] == 1:\n",
    "        cbd_cnt += 1\n",
    "    elif metadata['treatment'] == 2 or metadata['treatment'] == 3:\n",
    "        rgs_cnt += 1\n",
    "    elif metadata['treatment'] == 4:\n",
    "        os_cnt += 1\n",
    "\n",
    "assert cbd_cnt == 170\n",
    "assert rgs_cnt == 159\n",
    "assert os_cnt == 210"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop through the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7baacf7f424f25a970073ae01d6d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/539 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tqdm(mapped_datasets) as t:\n",
    "    for name in t:\n",
    "        metadata = get_metadata(name)\n",
    "        t.set_postfix_str(name)\n",
    "        states_fname, hpc_fname, pfc_fname = mapped_datasets[name]\n",
    "        logger.debug(\"Loading: {0}\".format(name))\n",
    "\n",
    "        if metadata[\"treatment\"] == 0 or metadata[\"treatment\"] == 1:\n",
    "            n_down = n_down_cbd\n",
    "        elif metadata[\"treatment\"] == 2 or metadata[\"treatment\"] == 3:\n",
    "            n_down = n_down_rgs\n",
    "        elif metadata[\"treatment\"] == 4:\n",
    "            n_down = n_down_os\n",
    "        \n",
    "        # Load the LFP data\n",
    "        lfpHPC = loadmat(hpc_fname)['HPC'].flatten()\n",
    "        lfpPFC = loadmat(pfc_fname)['PFC'].flatten()\n",
    "\n",
    "        # Load the states\n",
    "        hypno = loadmat(states_fname)['states'].flatten()\n",
    "        \n",
    "        # Skip if no REM epoch is detected\n",
    "        if(not (np.any(hypno == 5))):\n",
    "            logger.debug(\"No REM detected. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Create Pynapple IntervalSet        \n",
    "        start, end = get_start_end(hypno=hypno, sleep_state_id=5)\n",
    "        rem_interval = nap.IntervalSet(start=start, end=end)\n",
    "        \n",
    "        # Create TsdFrame for HPC and PFC signals\n",
    "        fs = n_down*targetFs\n",
    "        t = np.arange(0, len(lfpHPC)/fs, 1/fs)\n",
    "        lfp = nap.TsdFrame(t=t, d=np.vstack([lfpHPC, lfpPFC]).T, columns=['HPC', 'PFC'])\n",
    "        \n",
    "        # Detect phasic intervals\n",
    "        lfpHPC_down = preprocess(lfpHPC, n_down)\n",
    "        phREM = detect_phasic(lfpHPC_down, hypno, targetFs)\n",
    "\n",
    "        # Create phasic REM IntervalSet\n",
    "        start, end = [], []\n",
    "        for rem_idx in phREM:\n",
    "            for s, e in phREM[rem_idx]:\n",
    "                start.append(s/targetFs)\n",
    "                end.append(e/targetFs)\n",
    "        phasic_interval = nap.IntervalSet(start, end)\n",
    "\n",
    "        tonic_interval = rem_interval.set_diff(phasic_interval)\n",
    "        \n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access the HPC and PFC signals during phasic REM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time (s)         HPC         PFC\n",
       "----------  --------  ----------\n",
       "1379.984    -226.951    37.5347\n",
       "1379.9844   -217.58     59.8681\n",
       "1379.9848   -198.255    75.2631\n",
       "1379.9852   -206.673    78.1636\n",
       "1379.9856   -193.815   107.547\n",
       "1379.986    -156.379   168.706\n",
       "1379.9864   -206.15    118.607\n",
       "...\n",
       "2432.3056   -220.943   -27.4514\n",
       "2432.306    -206.43    -13.7936\n",
       "2432.3064   -235.411   -31.6098\n",
       "2432.3068   -273.278   -77.5986\n",
       "2432.3072   -283.825   -80.6341\n",
       "2432.3076   -203.913     6.07146\n",
       "2432.308    -315.641  -113.657\n",
       "dtype: float64, shape: (22574, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfp.restrict(phasic_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access HPC and PFC signals during tonic REM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time (s)           HPC       PFC\n",
       "----------  ----------  --------\n",
       "1331.0      -180.893    -93.2364\n",
       "1331.0004   -150.785    -69.5545\n",
       "1331.0008   -132.412    -70.9175\n",
       "1331.0012    -85.3764   -50.1542\n",
       "1331.0016    -79.2403   -50.5931\n",
       "1331.002     -75.6617   -49.3135\n",
       "1331.0024    -57.6323   -34.4635\n",
       "...\n",
       "2474.9976   -125.165     76.2955\n",
       "2474.998    -118.796     64.7644\n",
       "2474.9984     -4.49443  173.913\n",
       "2474.9988    -37.3338   133.221\n",
       "2474.9992   -137.777     25.3803\n",
       "2474.9996   -103.864     80.9616\n",
       "2475.0       -36.7785   134.176\n",
       "dtype: float64, shape: (479936, 2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfp.restrict(tonic_interval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
